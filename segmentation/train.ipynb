{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation - Scene Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HW3 - Segmentation\n",
    "\n",
    "### Come up with Story\n",
    "    0. You will work in groups of three. Come up with a team name.\n",
    "    \n",
    "    1. You are given powerful segmentation model, that simulates human annotator labelling the listed classes below\n",
    "    \n",
    "    2. Come up with the interesting (specific use-case) segmentation application on the images you can produce with your phone or gather from internet\n",
    "    \n",
    "    3. State in which situation it can be used, i.e. robot navigating through terrain, catching escaped animals from zoo, safari, collecting food from table etc.\n",
    "    \n",
    "    4. It can be related to your thesis as well\n",
    "    \n",
    "\n",
    "### Design Model\n",
    "    1. You are given MobileNetV3 as a segmentation architecture. You can use whichever you want, this is just recommended as it is running realtime on gpu and can be tested on cpu \n",
    "\n",
    "    2. You will train the model (you can use pretrained weights on different scenes) on your collected image data\n",
    "    \n",
    "    3. Split the data to training part and testing part and validate your model on testing part in terms of IoU from Teacher and visualized outputs\n",
    "    \n",
    "    4. Specify some unique scenarios for testing and show loss values and final segmentation on these cases\n",
    "        - Discuss how it fails or succeded\n",
    "        - Try to explain why and what you help to improve the performance on these cases\n",
    "\n",
    "### Things that can help you\n",
    "    - Strong Regularization\n",
    "        1. Weight decay in torch optimizer\n",
    "        2. Data augmentation\n",
    "        3. Using pretrained model\n",
    "        4. More training data from unique scenarios\n",
    "        5. (Advanced) self-supervised pre-training\n",
    "        \n",
    "    - Server GPUs \n",
    "        1. Taylor and Cantor - ssh username@taylor.felk.cvut.cz or ssh username@cantor.felk.cvut.cz\n",
    "        2. Video Tutorial in server.mp4\n",
    "        3. Text guide on: https://cyber.felk.cvut.cz/cs/study/gpu-servers/\n",
    "        \n",
    "### Final Presentation\n",
    "   \n",
    "    \n",
    "    1. Describe Idea in sheets: https://docs.google.com/spreadsheets/d/1rvsg9ZgzmXiVJsiJvnpsy-yQ7N5WqADn10NL235eC1M/edit?usp=sharing\n",
    "    \n",
    "    2. Evaluation will be given on the day of presentations - 17./18.12. based on your parralel \n",
    "    \n",
    "### Evaluation\n",
    "\n",
    "    1. Idea and preparation of data (Unique scenarios, Useful Teacher outputs, amount of training samples)\n",
    "    \n",
    "    2. Training - loss minimization, Validity of approach, Tweaks to training (Regularizations, Augmentation, ...)\n",
    "    \n",
    "    3. Examples and output overview and discussion\n",
    "    \n",
    "    4. Discussion of training times and speed of teacher and inference model. Is it sufficiently fast for the application?\n",
    "    \n",
    "    5. Presentation clarity and enthusiasm\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"username2/rgb/a.png\" width=\"480\"> <img src=\"username2/vis/a.png\" width=\"480\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Teacher Model - source of \"ground truth\" labels but very slow and only achievable with Facebook-level resources\n",
    "\n",
    "### Classes\n",
    "- from https://github.com/cocodataset/panopticapi/blob/master/panoptic_coco_categories.json\n",
    "\n",
    "- Try what you can segment and what not\n",
    "    - Or Try it directly on server\n",
    "    - https://segment-anything.com/demo\n",
    "\n",
    "- See on which data the foundation model was trained on\n",
    "    - https://cocodataset.org/#explore\n",
    "    \n",
    "### Running the teacher model\n",
    "   - Store images on Taylor server for annotations \n",
    "       - <mark>/local/temporary/UROB/segmentation/students/YOUR_TEAM_NAME/rgb/*.png</mark>\n",
    "   - Load necessary modules and Install libraries locally to the user profile \n",
    "       - source <mark>/local/temporary/UROB/segmentation/SEEM/demo_code/install.sh</mark>\n",
    "   - Run the teacher model\n",
    "       - python <mark>/local/temporary/UROB/segmentation/SEEM/demo_code/app.py --username YOUR_TEAM_NAME</mark>\n",
    "       - If you started new ssh session, you need to load modules in install.sh again. You can run first few commands or source it again.\n",
    "\n",
    "### See the results\n",
    "   - In <mark>/local/temporary/UROB/segmentation/students/YOUR_TEAM_NAME/seg/</mark>, you can have the segmentation output resized to <mark>512x512</mark> and the label is encoded in grayscale.\n",
    "   - The value of grayscale correspond with the index of name_list bellow\n",
    "   - You can access the segmentation class name by: <mark>name_list[i]</mark>, where \"i\" is the value of pixel\n",
    "   - In <mark>/local/temporary/UROB/segmentation/students/YOUR_TEAM_NAME/vis/</mark>, you see overlay visualization of the segmentation, that is humanly readable\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"username2/rgb/a.png\" width=\"400\"> <img src=\"username2/seg/a.png\" width=\"400\"> <img src=\"username2/vis/a.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'banner', 'blanket', 'bridge', 'cardboard', 'counter', 'curtain', 'door-stuff', 'floor-wood', 'flower', 'fruit', 'gravel', 'house', 'light', 'mirror-stuff', 'net', 'pillow', 'platform', 'playingfield', 'railroad', 'river', 'road', 'roof', 'sand', 'sea', 'shelf', 'snow', 'stairs', 'tent', 'towel', 'wall-brick', 'wall-stone', 'wall-tile', 'wall-wood', 'water-other', 'window-blind', 'window-other', 'tree-merged', 'fence-merged', 'ceiling-merged', 'sky-other-merged', 'cabinet-merged', 'table-merged', 'floor-other-merged', 'pavement-merged', 'mountain-merged', 'grass-merged', 'dirt-merged', 'paper-merged', 'food-other-merged', 'building-other-merged', 'rock-merged', 'wall-other-merged', 'rug-merged']\n",
    "\n",
    "print(name_list[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastseg --user\n",
    "from fastseg import MobileV3Small\n",
    "model = MobileV3Small.from_pretrained()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastseg.image.colorize import colorize, blend\n",
    "\n",
    "# Open image from file and resize it to lower memory footprint\n",
    "img = Image.open('image.png').resize((1024, 512))\n",
    "\n",
    "# Change the class from PIL.Image into numpy array\n",
    "img_np = np.asarray(img)\n",
    "\n",
    "# Create torch tensor from numpy array and add dimension representing batchsize. Also change dtype to float as it is required by torch\n",
    "x = torch.tensor(img_np).unsqueeze(0).float()\n",
    "\n",
    "# Transpose dimension of tensor so it respects the torch convention: Batch Size x Number of Classes x Height x Width\n",
    "x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "# Normalize data\n",
    "x = (x / 255) * 2 - 1\n",
    "\n",
    "# Forward pass, input image x and return output probabilities for each pixel and each class along each image in batch size\n",
    "output = model(x)\n",
    "\n",
    "# Output in \n",
    "print(\"Following is for the first pixel [0,0] of first image in batch [0]: \\n\")\n",
    "print('Logits: \\n', output[0,:,0,0], '\\n')\n",
    "print('Probabilities: \\n', output.softmax(dim=1)[0,:,0,0], '\\n')\n",
    "print('Prediction: \\n', output.argmax(dim=1)[0,0,0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of final segmentation prediction from class probabilities along dimension 1\n",
    "# detach.cpu.numpy transfer tensor from torch to computational graph-detached, to cpu memory and to numpy array instead of tensor\n",
    "seg_np = output.argmax(dim=1)[0].detach().cpu().numpy()\n",
    "\n",
    "# Function from fastseg to visualize images and output segmentation\n",
    "seg_img = colorize(seg_np) # <---- input is numpy, output is PIL.Image\n",
    "blended_img = blend(img, seg_img) # <---- input is PIL.Image in both arguments\n",
    "\n",
    "# Concatenate images for simultaneous view\n",
    "new_array = np.concatenate((np.asarray(blended_img), np.asarray(seg_img)), axis=1)\n",
    "\n",
    "# Show image from PIL.Image class\n",
    "combination = Image.fromarray(new_array)\n",
    "# combination.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Image - Output \n",
    "![alt text](input-output.png \"i/o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data\n",
    "- Shuffle the data every training loop to prevent overfitting on examples one by one\n",
    "- Feed the inputs in batches, so it can see \"the most\" of the things at once to prevent overfitting\n",
    "    - you can use https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "- Data Augmentation for extending training samples and therefore catch more configurations of objects\n",
    "    - https://pytorch.org/vision/stable/transforms.html\n",
    "    - https://pytorch.org/vision/0.12/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already implemented data augmentations in torch vision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Example of rotating input image to show different \"views\" on objects\n",
    "# will rotate the image and the segmentation mask differently!!!\n",
    "rotater = T.RandomRotation(degrees=(0, 180))\n",
    "orig_img = torch.from_numpy(np.asarray(Image.open('image.png').resize((1024, 512)))).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "# Just visualization\n",
    "seg_img = torch.from_numpy(seg_np).unsqueeze(0).float()\n",
    "Image.fromarray(rotater(seg_img).to(torch.uint8).numpy()[0]).show()\n",
    "# Just visualization\n",
    "rotated_img = rotater(orig_img).to(torch.uint8)\n",
    "Image.fromarray(rotated_img[0].permute(1,2,0).numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss module. Basically softmax + negative log-likelihood.\n",
    "# Softmax chains the output probabilities and set it between values 0-1\n",
    "# Negative log-likelihood makes the values behaves more smoothly and penalize values like 0.1 much more than 0.9.\n",
    "# weight argument sets the penalization per-class. When there is too much of the background, the model will overfit on majority class during training (background)\n",
    "# You should calculate the number of pixels per class to avoid the overfit and specify the ratio in CrossEntropyLoss torch module.\n",
    "CE = torch.nn.CrossEntropyLoss(reduction=\"none\", weight=None)\n",
    "\n",
    "# Get final prediction with argmax\n",
    "labels = output.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"neg_log.png\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, can be from pretrained version (prefered). Here it is for educational purposes\n",
    "num_classes = 19\n",
    "model = MobileV3Small(num_classes=num_classes)\n",
    "\n",
    "# Set up model to training mode (some layers are designed to behave differently during learning and during inference - batch norm for example.)\n",
    "# Always learn model in training mode\n",
    "model.train()\n",
    "\n",
    "# Set up optimizer to automatically update weights with respect to computed loss and negative of gradient\n",
    "# Regularization weight decay - analogy with remembering the exam questions\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "# Multiple iterations (epochs)\n",
    "for e in range(100):\n",
    "    \n",
    "    # Forward pass of model. Input image x and output as per-pixel probabilities, per-image in batch\n",
    "    # output dimensions: Batch Size x Class probs x H x W\n",
    "    output = model(x)\n",
    "    \n",
    "    # Calculation of Loss function, we use pytorch implementations of Cross entropy (softmax + negative log-likelihood)\n",
    "    loss = CE(output, labels)\n",
    "    \n",
    "    # Print loss and metric Intersection-over-union to monitor model's performance during the training\n",
    "    # Why there is non-zero loss when learning on the self-produced labels?\n",
    "    print(f'Epoch: {e:03d}', f'Loss: {loss.mean().item():.4f}')\n",
    "    \n",
    "    # This step is the most important. On the backend, Torch will accumulate gradients along the performed operations and keeps it in the memory\n",
    "    # After calling backward(), the gradients are recomputed for specific forward pass and the model accumulates gradients with respect to the loss\n",
    "    loss.mean().backward()\n",
    "    \n",
    "    # After we compute the gradients from backward(), each weight in the model will have the .grad value.\n",
    "    # Optimizer will then use the gradient and learning rate to update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Test if the models has accumulated gradients and therefore \"learn something\"\n",
    "    if e == 0:\n",
    "        print(\"Gradient in the last layer on specific weights: \", model.last.weight[0,0,0,0])\n",
    "        \n",
    "    # Clean already used gradients to start over in the new iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Visualization of model's output at every iterations\n",
    "    seg_np = output.argmax(dim=1)[0].detach().cpu().numpy()\n",
    "    seg_img = colorize(seg_np)\n",
    "    seg_img.save(f'overfitting/{e:03d}.png')\n",
    "\n",
    "\n",
    "# Saving weights\n",
    "torch.save(model.state_dict(), 'weights/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "model.load_state_dict(torch.load('weights/model.pth', map_location='cpu'))\n",
    "\n",
    "# Setting model to eval mode\n",
    "# Always test model in eval mode\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
