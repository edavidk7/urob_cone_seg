{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation - Scene Understanding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HW3 - Segmentation\n",
    "\n",
    "### Come up with Story\n",
    "\n",
    "    0. You will work in groups of three. Come up with a team name.\n",
    "\n",
    "    1. You are given powerful segmentation model, that simulates human annotator labelling the listed classes below\n",
    "\n",
    "    2. Come up with the interesting (specific use-case) segmentation application on the images you can produce with your phone or gather from internet\n",
    "\n",
    "    3. State in which situation it can be used, i.e. robot navigating through terrain, catching escaped animals from zoo, safari, collecting food from table etc.\n",
    "\n",
    "    4. It can be related to your thesis as well\n",
    "\n",
    "### Design Model\n",
    "\n",
    "    1. You are given MobileNetV3 as a segmentation architecture. You can use whichever you want, this is just recommended as it is running realtime on gpu and can be tested on cpu\n",
    "\n",
    "    2. You will train the model (you can use pretrained weights on different scenes) on your collected image data\n",
    "\n",
    "    3. Split the data to training part and testing part and validate your model on testing part in terms of IoU from Teacher and visualized outputs\n",
    "\n",
    "    4. Specify some unique scenarios for testing and show loss values and final segmentation on these cases\n",
    "        - Discuss how it fails or succeded\n",
    "        - Try to explain why and what you help to improve the performance on these cases\n",
    "\n",
    "### Things that can help you\n",
    "\n",
    "    - Strong Regularization\n",
    "        1. Weight decay in torch optimizer\n",
    "        2. Data augmentation\n",
    "        3. Using pretrained model\n",
    "        4. More training data from unique scenarios\n",
    "        5. (Advanced) self-supervised pre-training\n",
    "\n",
    "    - Server GPUs\n",
    "        1. Taylor and Cantor - ssh username@taylor.felk.cvut.cz or ssh username@cantor.felk.cvut.cz\n",
    "        2. Video Tutorial in server.mp4\n",
    "        3. Text guide on: https://cyber.felk.cvut.cz/cs/study/gpu-servers/\n",
    "\n",
    "### Final Presentation\n",
    "\n",
    "    1. Describe Idea in sheets: https://docs.google.com/spreadsheets/d/1rvsg9ZgzmXiVJsiJvnpsy-yQ7N5WqADn10NL235eC1M/edit?usp=sharing\n",
    "\n",
    "    2. Evaluation will be given on the day of presentations - 17./18.12. based on your parralel\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "    1. Idea and preparation of data (Unique scenarios, Useful Teacher outputs, amount of training samples)\n",
    "\n",
    "    2. Training - loss minimization, Validity of approach, Tweaks to training (Regularizations, Augmentation, ...)\n",
    "\n",
    "    3. Examples and output overview and discussion\n",
    "\n",
    "    4. Discussion of training times and speed of teacher and inference model. Is it sufficiently fast for the application?\n",
    "\n",
    "    5. Presentation clarity and enthusiasm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "try:\n",
    "    plt.rcParams[\"text.usetex\"] = True\n",
    "    plt.rcParams[\"font.family\"] = \"Apple Roboto\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fsoco_segmentation_processed\"\n",
    "dataset_dir = Path(dataset)\n",
    "imgs_dir = dataset_dir / \"img\"\n",
    "masks_dir = dataset_dir / \"ann\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_imgs = list(imgs_dir.glob(\"*.jpeg\"))\n",
    "processed_masks = list(masks_dir.glob(\"*.npz\"))\n",
    "processed_imgs.sort()\n",
    "processed_masks.sort()\n",
    "img_mask_pairs = list(zip(processed_imgs, processed_masks))\n",
    "img_mask_pairs[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class and augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_color_jitter = {\n",
    "    0: {\"hue\": 0.5, \"saturation\": 0.5},\n",
    "    1: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "    2: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "    3: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "    4: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "    5: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "}\n",
    "\n",
    "T = transforms.Compose(\n",
    "    [\n",
    "        ResizeWithMask(size=(700, 2000), antialias=True),\n",
    "        RandomCropWithMask(size=(512, 512)),\n",
    "        Normalize(),\n",
    "        RandomHorizontalFlipWithMask(0.5),\n",
    "        RandomAffineWithMask(degrees=10, translate=(0.01, 0.01)),\n",
    "        RandomRotationWithMask(\n",
    "            degrees=5),\n",
    "        ClasswiseColorJitter(class_color_jitter)\n",
    "    ])\n",
    "\n",
    "dataset = ConeSegmentationDataset(img_mask_pairs, transform=T)\n",
    "\n",
    "for i in range(3):\n",
    "    test_img, test_mask = dataset[i]\n",
    "    print(test_img.max(), test_img.min(), test_mask.max(), test_mask.min())\n",
    "    visualize_from_torch(test_img, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmask_iou(test_mask.unsqueeze(0), test_mask.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch config\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset splits\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "\n",
    "# Transforms for training\n",
    "class_color_jitter = {\n",
    "    0: {\"hue\": 0.5, \"saturation\": 0.5},\n",
    "    1: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "    2: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "    3: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "    4: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "    5: {\"hue\": 0.5, \"saturation\": 0.8},\n",
    "}\n",
    "\n",
    "train_T = transforms.Compose(\n",
    "    [\n",
    "        ResizeWithMask(size=(700, 2000), antialias=True),\n",
    "        RandomCropWithMask(size=(512, 512)),\n",
    "        Normalize(),\n",
    "        RandomHorizontalFlipWithMask(0.5),\n",
    "        RandomAffineWithMask(degrees=10, translate=(0.01, 0.01)),\n",
    "        RandomRotationWithMask(\n",
    "            degrees=5),\n",
    "        ClasswiseColorJitter(class_color_jitter)\n",
    "    ])\n",
    "\n",
    "eval_T = transforms.Compose([Normalize()])\n",
    "\n",
    "# Â Split img-mask pairs\n",
    "train_size = int(TRAIN_SIZE * len(img_mask_pairs))\n",
    "val_size = int(VAL_SIZE * len(img_mask_pairs))\n",
    "test_size = len(img_mask_pairs) - train_size - val_size\n",
    "train_pairs, val_pairs, test_pairs = torch.utils.data.random_split(\n",
    "    img_mask_pairs, [train_size, val_size, test_size])\n",
    "\n",
    "# Create datasets\n",
    "train_set = ConeSegmentationDataset(train_pairs, transform=train_T)\n",
    "val_set = ConeSegmentationDataset(val_pairs, transform=eval_T)\n",
    "test_set = ConeSegmentationDataset(test_pairs, transform=eval_T)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=2,\n",
    "                          shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastseg import MobileV3Large, MobileV3Small\n",
    "# model = MobileV3Large(num_classes=6, use_aspp=True, num_filters=128)\n",
    "model = MobileV3Small(num_classes=6, use_aspp=True, num_filters=128)\n",
    "model = model.to(device)\n",
    "CE = torch.nn.CrossEntropyLoss(reduction=\"none\", weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "EPOCHS = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "for e in range(EPOCHS):\n",
    "    loop = tqdm.tqdm(train_loader)\n",
    "    for x, labels in loop:\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(x)\n",
    "        loss = CE(output, labels)\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_description(f\"Epoch {e} loss: {loss.mean().item():.4f}\")\n",
    "\n",
    "# Saving weights\n",
    "# torch.save(model.state_dict(), 'weights/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = Path('weights')\n",
    "weights_path.mkdir(exist_ok=True)\n",
    "torch.save(model.state_dict(), weights_path / 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "model.to(\"cpu\")\n",
    "model.load_state_dict(torch.load('weights/model.pth', map_location='cpu'))\n",
    "\n",
    "# Setting model to eval mode\n",
    "# Always test model in eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, ty = test_set[0]\n",
    "t = t.unsqueeze(0)\n",
    "output = model(t)\n",
    "output = output.argmax(1).squeeze().cpu().numpy()\n",
    "visualize_from_torch(t.squeeze().cpu(), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = torch.rand(32, 6, 512, 512, device=\"mps\")\n",
    "test_b = torch.rand(32, 6, 512, 512, device=\"mps\")\n",
    "segmask_iou(test_a, test_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
